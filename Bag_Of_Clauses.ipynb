{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNdgPXfVlV+EF6wQb8jgHtb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RichardMWarburton/ExploringCUAD/blob/Dev/Bag_Of_Clauses.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hmzq-iERY3pV"
      },
      "source": [
        "#Building a Bag of Clauses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZjFbqMRY46k"
      },
      "source": [
        "## 1: Import Packages & Define Useful Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WxXJaRVYdBt",
        "outputId": "48efcf2a-919e-429f-9550-7888ee8bd704"
      },
      "source": [
        "from zipfile import ZipFile\n",
        "import json\n",
        "import os\n",
        "from collections import Counter, defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from random import sample, choice\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import DBSCAN, KMeans\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from pprint import pprint\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "!pip install dateparser\n",
        "import dateparser\n",
        "\n",
        "!pip install num2words\n",
        "from num2words import num2words\n",
        "\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from collections import Counter\n",
        "import en_core_web_sm\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "Collecting dateparser\n",
            "  Downloading dateparser-1.0.0-py2.py3-none-any.whl (279 kB)\n",
            "\u001b[K     |████████████████████████████████| 279 kB 29.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from dateparser) (1.5.1)\n",
            "Requirement already satisfied: regex!=2019.02.19 in /usr/local/lib/python3.7/dist-packages (from dateparser) (2019.12.20)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from dateparser) (2.8.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from dateparser) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil->dateparser) (1.15.0)\n",
            "Installing collected packages: dateparser\n",
            "Successfully installed dateparser-1.0.0\n",
            "Collecting num2words\n",
            "  Downloading num2words-0.5.10-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 8.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.7/dist-packages (from num2words) (0.6.2)\n",
            "Installing collected packages: num2words\n",
            "Successfully installed num2words-0.5.10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gON4BIcUZCZt"
      },
      "source": [
        "def extract_zip(pth,data_pth = None):\n",
        "    \"\"\"Function to extract contents of a zip file to a specified location (wd if data_pth not passed)\"\"\"\n",
        "    with ZipFile(pth, 'r') as zipObj:\n",
        "       # Extract all the contents of zip file in different directory\n",
        "       zipObj.extractall(data_pth)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12Ar2QNvb8-Y"
      },
      "source": [
        "def lower_case(x):\n",
        "  \"\"\"Function to return a lowecase value of a string x\"\"\"\n",
        "  return x.lower()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCe9h76lb9jW"
      },
      "source": [
        "def remove_stop_words(x):\n",
        "  \"\"\"Function to remove stop words from a string x \"\"\"  \n",
        "  return ' '.join([word for word in x.split(sep=' ') if word not in stop_words])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2yHBbfGb9my"
      },
      "source": [
        "def coerce_tokens(x):\n",
        "  ret = x.replace('new york','newyork')\n",
        "  ret = ret.replace('new jersey','newjersey')\n",
        "  ret = ret.replace('hong kong','hongkong')\n",
        "  ret = ret.replace('san antonio','sanantonio')\n",
        "  return ret"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFmaUZILiun0"
      },
      "source": [
        "def append_type_match(x, value_list,matches=[]):\n",
        "  if x.label_ in matches:\n",
        "    value_list.append(x)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwMyQOBmizWN"
      },
      "source": [
        "def ordinalize_num(txt):\n",
        "  for token in txt.split(sep=' '):\n",
        "    try:\n",
        "      if int(token) <= 31:\n",
        "        ord_val = num2words(token, lang=\"en\", to=\"ordinal_num\")\n",
        "        txt = txt.replace(token,ord_val)\n",
        "    except:\n",
        "      continue\n",
        "  return txt"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RxcXsbqZIcN"
      },
      "source": [
        "## 2: Download repository and extract data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQOJy9rwZRXi"
      },
      "source": [
        "### 2.1: Extract Raw Contract Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7X14ziVZEYq",
        "outputId": "a04f24e6-dd1e-467f-b149-c6361704db91"
      },
      "source": [
        "#Download CUAD git repository\n",
        "if not os.path.exists('main.zip'):\n",
        "  !wget --no-check-certificate https://github.com/TheAtticusProject/cuad/archive/refs/heads/main.zip\n",
        "  !unzip -q main.zip\n",
        "\n",
        "#If it has not already been extracted, extract the contents of data.zip\n",
        "if not os.path.exists('cuad-main/data'):\n",
        "  os.makedirs('cuad-main/data')\n",
        "\n",
        "if not os.path.exists('cuad-main/data/CUADv1.json'):\n",
        "  extract_zip('cuad-main/data.zip','cuad-main/data/')\n",
        "\n",
        "#Download a manualy curated set of labels for the full CUAD data. \n",
        "if not os.path.exists('labels3.txt'):\n",
        "  !wget https://raw.githubusercontent.com/RichardMWarburton/ExploringCUAD/main/labels3.txt"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-20 16:00:19--  https://github.com/TheAtticusProject/cuad/archive/refs/heads/main.zip\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://codeload.github.com/TheAtticusProject/cuad/zip/refs/heads/main [following]\n",
            "--2021-07-20 16:00:19--  https://codeload.github.com/TheAtticusProject/cuad/zip/refs/heads/main\n",
            "Resolving codeload.github.com (codeload.github.com)... 140.82.114.10\n",
            "Connecting to codeload.github.com (codeload.github.com)|140.82.114.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘main.zip’\n",
            "\n",
            "main.zip                [  <=>               ]  17.77M  22.1MB/s    in 0.8s    \n",
            "\n",
            "2021-07-20 16:00:20 (22.1 MB/s) - ‘main.zip’ saved [18631176]\n",
            "\n",
            "--2021-07-20 16:00:21--  https://raw.githubusercontent.com/RichardMWarburton/ExploringCUAD/main/labels3.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 47879 (47K) [text/plain]\n",
            "Saving to: ‘labels3.txt’\n",
            "\n",
            "labels3.txt         100%[===================>]  46.76K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2021-07-20 16:00:21 (33.5 MB/s) - ‘labels3.txt’ saved [47879/47879]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jb19qaiZZLw6"
      },
      "source": [
        "#Load CUADv1 JSON to data\n",
        "with open('cuad-main/data/CUADv1.json','r') as infile:\n",
        "    for line in infile:\n",
        "        contract_data = json.loads(line)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eK5g7_3UZQbD"
      },
      "source": [
        "#ser reg ex expression for characters to remove from contract contest\n",
        "spec_chars = '\\\\n|\\\\t|\\\\t'\n",
        "\n",
        "#Set number of contracts in data\n",
        "num_contracts = len(contract_data['data'])\n",
        "\n",
        "#Initate dictionary to store raw contract data\n",
        "raw_contracts = defaultdict(list)\n",
        "\n",
        "#for each contract\n",
        "for i in range(num_contracts):\n",
        "  #Append the title, contract text and character length of text to the raw_contracts dictionary\n",
        "  raw_contracts['contract title'].append(contract_data['data'][i]['title'])\n",
        "  #raw_contracts['label'].append(labels_LU[contract_data['data'][i]['title']] if contract_data['data'][i]['title'] in labels_LU else 'marketing agreement' ) #<- manual error trap applied here (see below)\n",
        "  \n",
        "  #Parse raw text and process to remove breaks\n",
        "  raw_text = contract_data['data'][i]['paragraphs'][0]['context']\n",
        "  clean_text = re.sub(spec_chars,'',raw_text)\n",
        "\n",
        "  #Split clean text in to sentances and tokens\n",
        "  sentance_text = clean_text.split(sep = '. ')\n",
        "  token_text = clean_text.split(sep = ' ')\n",
        "\n",
        "  #Append text to the respective key in the raw_contracts dictionary\n",
        "  raw_contracts['raw text'].append(raw_text)\n",
        "  raw_contracts['clean text'].append(clean_text)\n",
        "  raw_contracts['sentance text'].append(sentance_text)\n",
        "  raw_contracts['token text'].append(token_text)\n",
        "  \n",
        "  #Add character, sentance and token counts to raw_contracts dictionary\n",
        "  raw_contracts['character count'].append(len(raw_text))\n",
        "  raw_contracts['sentance count'].append(len(sentance_text))\n",
        "  raw_contracts['token count'].append(len(token_text))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOZX7BMyZrXB"
      },
      "source": [
        "### 2.2: Extract Clause Specific Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dp0Jeww3Zbh6"
      },
      "source": [
        "#Define the number of clauses\n",
        "num_clauses = 41\n",
        "\n",
        "#initate dictioanry to store caluse data\n",
        "clause_data = defaultdict(list)\n",
        "\n",
        "#For each contract\n",
        "for i in range(num_contracts):\n",
        "  #for each clause\n",
        "  for j in range(num_clauses):\n",
        "    #for each found clause annotation\n",
        "    for k in range(len(contract_data['data'][i]['paragraphs'][0]['qas'][j]['answers'])): \n",
        "      #Add the contract title\n",
        "      clause_data['contract title'].append(contract_data['data'][i]['title'])\n",
        "      #clause_data['label'].append(labels_LU[contract_data['data'][i]['title']] if contract_data['data'][i]['title'] in labels_LU else 'marketing agreement' )  #<- manual error trap applied here\n",
        "      clause_data['clause'].append(contract_data['data'][i]['paragraphs'][0]['qas'][j]['id'].split(sep='__')[1])\n",
        "      clause_data['annotation'].append(contract_data['data'][i]['paragraphs'][0]['qas'][j]['answers'][k]['text'])\n",
        "      clause_data['annotation start'].append(contract_data['data'][i]['paragraphs'][0]['qas'][j]['answers'][k]['answer_start'])\n",
        "      clause_data['annotation length'].append(len(contract_data['data'][i]['paragraphs'][0]['qas'][j]['answers'][k]['text']))\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSbz7f_9Z5v4"
      },
      "source": [
        "### 2.3: Cleaning Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpffuCh0Z0oK"
      },
      "source": [
        "#Initate dataframe of all clause data\n",
        "clause_df = pd.DataFrame(clause_data)\n",
        "\n",
        "#Convert to lower case\n",
        "#clause_df['annotation'] = clause_df['annotation'].apply(lambda x: x.lower())\n",
        "\n",
        "#Remove any formating characters or multiple spaces and replace with a single space\n",
        "clause_df['annotation'] = clause_df['annotation'].apply(lambda x: re.sub('\\\\t|\\\\r|\\\\n|[^\\S]{2,}',' ',x))\n",
        "\n",
        "#Remove punctuation from the string\n",
        "clause_df['annotation'] = clause_df['annotation'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxN-LES3gRl0"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rR7A73gCac1S"
      },
      "source": [
        "## 3: Bag of Clauses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbRm_6Hef6St",
        "outputId": "23e8427c-e3f9-4ed2-d8c3-f9f730126f04"
      },
      "source": [
        "#Initate Bag of Clauses.\n",
        "bag_of_clauses = defaultdict(dict)\n",
        "\n",
        "#For each contract:\n",
        "for contract in np.unique(clause_df['contract title'].values):\n",
        "  idx = clause_df['contract title'] == contract\n",
        "  for clause in np.unique(clause_df[idx]['clause']):\n",
        "    bag_of_clauses[contract][clause] = np.nan\n",
        "\n",
        "bag_of_clauses['XYBERNAUTCORP_07_12_2002-EX-4-SPONSORSHIP AGREEMENT']"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Agreement Date': nan,\n",
              " 'Anti-Assignment': nan,\n",
              " 'Cap On Liability': nan,\n",
              " 'Document Name': nan,\n",
              " 'Exclusivity': nan,\n",
              " 'Expiration Date': nan,\n",
              " 'Governing Law': nan,\n",
              " 'License Grant': nan,\n",
              " 'Non-Disparagement': nan,\n",
              " 'Non-Transferable License': nan,\n",
              " 'Parties': nan}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TH0EILvVbGzE"
      },
      "source": [
        "### 3.1 Add 'Governeing Law' To BoC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHkMxpm9aE-D",
        "outputId": "71fd129a-d51c-4a3f-b821-42e364e64446"
      },
      "source": [
        "#Define clause of interest\n",
        "clause_of_interest = 'Governing Law'\n",
        "\n",
        "#Limit df to clause of interest and extract annotations of itnerest\n",
        "of_interest_data = clause_df[clause_df['clause'] == clause_of_interest]\n",
        "annotations_of_interest = of_interest_data['annotation'].values\n",
        "\n",
        "#Identify where there are multiple annotations per contract\n",
        "titles,counts = np.unique(of_interest_data['contract title'],return_counts =True)\n",
        "dups = titles[counts >= 2]\n",
        "\n",
        "#Output Analysis\n",
        "print('There are {} contracts with \\'{}\\' annotations'.format(*(titles.shape[0],clause_of_interest)))\n",
        "print('There are {} contracts with more than one annotation'.format(dups.shape[0]))\n",
        "\n",
        "#output duplicate annotations anc contract titles\n",
        "dup_df = of_interest_data[of_interest_data['contract title'].isin(dups)][['contract title','annotation']]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 437 contracts with 'Governing Law' annotations\n",
            "There are 25 contracts with more than one annotation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMqviYGJaFAX"
      },
      "source": [
        "#Initate memory for annotations within contracts\n",
        "combined_annotations_list = defaultdict(list)\n",
        "combined_annotations_string = {}\n",
        "\n",
        "#For each annotation of interest found in the contract, \n",
        "for i in of_interest_data.index:\n",
        "  #Append annotation to a default dict list with contract as key\n",
        "  name = of_interest_data.loc[i,['contract title']].values[0]\n",
        "  annotation = of_interest_data.loc[i,['annotation']].values[0]\n",
        "  combined_annotations_list[name].append(annotation) #<- coerces duplicate annotations in to a single string\n",
        "\n",
        "#Produce a single string of all annotations found in specific contracts\n",
        "for key in combined_annotations_list.keys():\n",
        "  combined_annotations_string[key] = ' '.join(combined_annotations_list[key])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjXlg46FaFC8"
      },
      "source": [
        "#Build array of contract names and concatenated annotations\n",
        "contracts = np.array(list(combined_annotations_string.keys()))\n",
        "combined_annotations = list(combined_annotations_string.values())"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oc3Xz_kcYN5"
      },
      "source": [
        "#### 3.1.1 Extract Governing Law via named entity recognition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8g50Lq6b3BH"
      },
      "source": [
        "#Define stopwords list\n",
        "stop_words = stopwords.words('english') +['hereof','thereof']\n",
        "\n",
        "nlp = en_core_web_sm.load()\n",
        "\n",
        "#Build new features\n",
        "new_features = []\n",
        "\n",
        "for annotation in combined_annotations:\n",
        "  annotation_GPEs = []\n",
        "  \n",
        "  doc = nlp(annotation)\n",
        "\n",
        "  for X in doc.ents:\n",
        "    #First pass to capture entities\n",
        "    if X.label_ == 'GPE' or X.label_ == 'LANGUAGE':\n",
        "      if X.text not in annotation_GPEs:\n",
        "        annotation_GPEs.append(X.text)\n",
        "  \n",
        "  #If no entities found, foce to lower case and re analyise\n",
        "  if annotation_GPEs == []:\n",
        "    doc = nlp(annotation.lower())\n",
        "\n",
        "    for X in doc.ents:\n",
        "      if X.label_ == 'GPE' or X.label_ == 'LANGUAGE':\n",
        "        if X.text not in annotation_GPEs:\n",
        "          annotation_GPEs.append(X.text)\n",
        "\n",
        "  new_features.append(' '.join(annotation_GPEs))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiB52j-icf6o"
      },
      "source": [
        "#initiate new_features as an np array\n",
        "new_features = np.array(new_features)\n",
        "\n",
        "#Clean New Features\n",
        "new_features = np.array(list(map(lower_case,new_features)))\n",
        "new_features = np.array(list(map(remove_stop_words,new_features)))\n",
        "new_features = np.array(list(map(coerce_tokens,new_features)))\n",
        "\n",
        "#Tokenize new_features\n",
        "#new_features = np.array(list((map(nltk.word_tokenize,new_features))),dtype=object)\n",
        "\n",
        "#Count Vectorize new_feature vector\n",
        "cv = CountVectorizer(stop_words = 'english')\n",
        "vectorized_features = cv.fit_transform(new_features).toarray()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgC6JGL4clsf"
      },
      "source": [
        "#Add Vectorization to BoC\n",
        "for i in range(len(contracts)):\n",
        "  bag_of_clauses[contracts[i]][clause_of_interest] = vectorized_features[i,:]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdC1BFwQiYnA"
      },
      "source": [
        "### 3.1 Add 'Agreement Date' To BoC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SruUXOq3iX2o",
        "outputId": "910f97e3-52eb-4a37-8831-291609b01a8a"
      },
      "source": [
        "#Define clause of interest\n",
        "clause_of_interest = 'Agreement Date'\n",
        "\n",
        "#Limit df to clause of interest and extract annotations of itnerest\n",
        "of_interest_data = clause_df[clause_df['clause'] == clause_of_interest]\n",
        "annotations_of_interest = of_interest_data['annotation'].values\n",
        "\n",
        "#Identify where there are multiple annotations per contract\n",
        "titles,counts = np.unique(of_interest_data['contract title'],return_counts =True)\n",
        "dups = titles[counts >= 2]\n",
        "\n",
        "#Output Analysis\n",
        "print('There are {} contracts with \\'{}\\' annotations'.format(*(titles.shape[0],clause_of_interest)))\n",
        "print('There are {} contracts with more than one annotation'.format(dups.shape[0]))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 470 contracts with 'Agreement Date' annotations\n",
            "There are 6 contracts with more than one annotation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6NNlEeAdLAg"
      },
      "source": [
        "#Initate memory for annotations within contracts\n",
        "combined_annotations_list = defaultdict(list)\n",
        "combined_annotations_string = {}\n",
        "\n",
        "#For each annotation of interest found in the contract, \n",
        "#append annotation to a default dict list with contract as key\n",
        "for i in of_interest_data.index:\n",
        "  name = of_interest_data.loc[i,['contract title']].values[0]\n",
        "  annotation = of_interest_data.loc[i,['annotation']].values[0]\n",
        "  combined_annotations_list[name].append(annotation)\n",
        "\n",
        "#Produce a singel string of all annotations found in specific contracts\n",
        "for key in combined_annotations_list.keys():\n",
        "  combined_annotations_string[key] = ' '.join(combined_annotations_list[key])\n",
        "\n",
        "#Build array of contract names and concatenated annotations\n",
        "contracts = np.array(list(combined_annotations_string.keys()))\n",
        "combined_annotations = np.array(list(combined_annotations_string.values()))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpWia1ukdbY-"
      },
      "source": [
        "date_feature_list = []\n",
        "not_mapped = []\n",
        "\n",
        "nlp = en_core_web_sm.load()\n",
        "\n",
        "for annotation in combined_annotations:\n",
        "\n",
        "  annotation_list = []  \n",
        "  #Clean annotaiton (typos etc)\n",
        "  annotation = annotation.replace('  ',' ')\n",
        "  annotation = ' '.join([word for word in annotation.split(sep=' ') if word not in stop_words])\n",
        "  annotation = annotation.replace('t h','th')\n",
        "  annotation = annotation.replace('s t','st')\n",
        "  annotation = annotation.replace('day','')\n",
        "  annotation = ordinalize_num(annotation)\n",
        "\n",
        "  doc = nlp(repr(annotation))\n",
        "\n",
        "  for X in doc.ents:\n",
        "    if X.label_ in ['DATE']:\n",
        "      annotation_list.append(X.text)\n",
        "    else:\n",
        "      annotation_list.append(X.text)\n",
        "      not_mapped.append(annotation)\n",
        "\n",
        "  date_feature_list.append(' '.join(annotation_list))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9RMLC0Ci6F7",
        "outputId": "44f40ece-6fed-49ab-bba8-4900c535508c"
      },
      "source": [
        "pass_vals = list(map(dateparser.parse,date_feature_list))\n",
        "matched = sum(list(map(lambda x: 1 if x == None else 0,pass_vals)))\n",
        "print('Pass 1: {} of {} not matched ({:.2%})'.format(*(matched,len(pass_vals),matched/len(pass_vals))))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pass 1: 66 of 470 not matched (14.04%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xpPGM5AoQy5"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKmOATNli6Lo",
        "outputId": "f8cefbd8-cf6f-41d0-acf2-bec0403d8446"
      },
      "source": [
        "#Remove any years significantly in the future (assumed errors)\n",
        "for i in range(len(pass_vals)):\n",
        "  if pass_vals[i] != None:\n",
        "    if pass_vals[i].year >2025:\n",
        "      print(pass_vals[i])\n",
        "      pass_vals[i] = None"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9997-07-20 00:00:00\n",
            "3411-07-20 00:00:00\n",
            "7798-07-20 00:00:00\n",
            "8410-07-20 00:00:00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgQfrYT-i6On"
      },
      "source": [
        "for i in range(len(contracts)):\n",
        "  if pass_vals[i] != None:\n",
        "    bag_of_clauses[contracts[i]][clause_of_interest] = pass_vals[i].timestamp()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qN9yxiWNi6Rh",
        "outputId": "29ba29b5-34a8-4592-c195-f693ff7331b4"
      },
      "source": [
        "bag_of_clauses['XYBERNAUTCORP_07_12_2002-EX-4-SPONSORSHIP AGREEMENT']"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Agreement Date': 1020211200.0,\n",
              " 'Anti-Assignment': nan,\n",
              " 'Cap On Liability': nan,\n",
              " 'Document Name': nan,\n",
              " 'Exclusivity': nan,\n",
              " 'Expiration Date': nan,\n",
              " 'Governing Law': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 1, 0, 0, 0]),\n",
              " 'License Grant': nan,\n",
              " 'Non-Disparagement': nan,\n",
              " 'Non-Transferable License': nan,\n",
              " 'Parties': nan}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_fjbhbdjP8z"
      },
      "source": [
        ""
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7qBYdlfoAHi"
      },
      "source": [
        "### 3.2 Expiration Date"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iX8qGh3FoCoQ",
        "outputId": "3f86f7d9-5c23-4de8-b618-d0ceabaa95ef"
      },
      "source": [
        "clause_data['clause'] == 'Audit Rights'"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0AokcuXqmDV",
        "outputId": "8f639083-bed5-4a66-e3df-a034c5522fb5"
      },
      "source": [
        "clause_df[clause_df['clause'] == 'Audit Rights']['annotation'][:2].values\n",
        "                                                          "
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['During the Term and for a period of    thereafter Google may audit Distributors relevant records to confirm Distributors compliance with this Agreement',\n",
              "       'Dova shall have the right at its own expense during normal business hours and upon reasonable prior notice through a certified public accounting firm or other auditor selected by Dova and reasonably acceptable to Valeant and upon execution of a confidentiality agreement reasonably satisfactory to Valeant in form and substance to inspect and audit the applicable records and books maintained by Valeant relating to the Valeant Activities for purposes of verifying Valeants compliance with the terms of this Agreement provided that i such examination shall not take place more often than once per every twelve 12 months during the Term and once during the one 1 year period following the end of the Term and ii such examination shall not cover a period of time that has previously been audited provided that Dova shall have the right to conduct additional for cause audits to the extent necessary to address significant compliance problems relating to Valeants obligations hereunder or in response to any inquiry inspection investigation or other requirements of a Government Authority in the Territory relating to the Valeant Activities'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvDMRiMxqnlu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}